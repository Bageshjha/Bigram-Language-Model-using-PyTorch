# Bigram-Language-Model-using-PyTorch
This repository contains an implementation of a Bigram Language Model built using PyTorch. The model generates text based on input sequences from a corpus (in this case, the text of The Wizard of Oz). The model is trained to predict the next character in a sequence of characters, which allows it to generate novel text after training.

Features-:

Tokenization and encoding of text data into integer sequences.

PyTorch-based Bigram Language Model implementation.

Model training using AdamW optimizer.

Ability to generate new text based on a context sequence.

Evaluation of train and validation loss during training.
